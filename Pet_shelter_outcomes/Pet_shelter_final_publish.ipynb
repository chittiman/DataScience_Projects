{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shelter Animal Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Every year, approximately 7.6 million companion animals end up in US shelters. Many animals are given up as unwanted by their owners, while others are picked up after getting lost or taken out of cruelty situations. Many of these animals find forever families to take them home, but just as many are not so lucky. 2.7 million dogs and cats are euthanized in the US every year.\n",
    "\n",
    "Using a dataset of information including breed, color, sex, and age , we need to predict the outcome for each animal.Submissions are evaluated using the multi-class logarithmic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the training and test dataset in the form of dataframe using read_csv function. Since DateTime column contains date and time , we have passed the column name to parse_dates so that it is stored as Datetime datatype instead of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('train.csv',parse_dates=['DateTime'])\n",
    "df_test = pd.read_csv('test.csv',parse_dates=['DateTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataframe ,  we are printing the datframe shape, null values and information about datetime column. Since the time duration for both the training and test datasets overlap we can divide the df_raw into training and validation set randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26729, 10)\n",
      "count                   26729\n",
      "unique                  22918\n",
      "top       2015-08-11 00:00:00\n",
      "freq                       19\n",
      "first     2013-10-01 09:31:00\n",
      "last      2016-02-21 19:17:00\n",
      "Name: DateTime, dtype: object\n",
      "\n",
      "\n",
      "Name               7691\n",
      "OutcomeSubtype    13612\n",
      "SexuponOutcome        1\n",
      "AgeuponOutcome       18\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "(11456, 8)\n",
      "count                   11456\n",
      "unique                  10575\n",
      "top       2014-10-20 09:00:00\n",
      "freq                        8\n",
      "first     2013-10-01 10:44:00\n",
      "last      2016-02-21 18:37:00\n",
      "Name: DateTime, dtype: object\n",
      "\n",
      "\n",
      "Name              3225\n",
      "AgeuponOutcome       6\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df in [df_raw,df_test]:\n",
    "    print(df.shape)\n",
    "    print(df.DateTime.describe())\n",
    "    print('\\n')\n",
    "    print(df.isnull().sum()[df.isnull().sum() != 0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are combining the training and test dataset , to perform the preprocessing. Also since the name column is not useful it is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.concat([df_raw,df_test],sort = True)\n",
    "\n",
    "df_comb.drop('Name',axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutered Male    14014\n",
       "Spayed Female    12633\n",
       "Intact Female     5004\n",
       "Intact Male       4985\n",
       "Unknown           1548\n",
       "NaN                  1\n",
       "Name: SexuponOutcome, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comb.SexuponOutcome.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value_counts() method gives the counts of unique values for a given column. Since that row with one unknown value is in training set we are dropping it. The Unknown value is replaced with the most frequent value of the SexuponOutcome  column.\\\n",
    "Similarly for AgeuponOutcome column the mode and median value was found to be the same i.e 1 year and hence the NaN values are filled with this mode.\n",
    "\n",
    "Now, we are dividing the each string in SexuponOutcome into two parts. 2nd part is assigned to the Sex and the 1st  part is assigned to the fertility. Ex: Neutered Male -> Neutered + Male.\n",
    "\n",
    "Age Column is split into 2 parts and using age_dict years, months etc are converted into the corresponding weeks and then multiplied with the integer part to get the total weeks. Ex .2 years -> 2 + years -> 2*52 -> 104 weeks.\n",
    "\n",
    "Then Age_weeks column is split into 10 equal groups using qcut function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb.dropna(subset = ['SexuponOutcome'],inplace = True)\n",
    "df_comb['SexuponOutcome'] = df_comb['SexuponOutcome'].replace('Unknown','Neutered Male')\n",
    "\n",
    "df_comb['AgeuponOutcome'].value_counts(dropna = False)\n",
    "df_comb['AgeuponOutcome'] = df_comb['AgeuponOutcome'].fillna(df_comb['AgeuponOutcome'].mode()[0])\n",
    "\n",
    "df_comb['Sex'] = df_comb.SexuponOutcome.str.split().str[1]\n",
    "\n",
    "df_comb['Fertility'] = df_comb.SexuponOutcome.str.split().str[0]\n",
    "\n",
    "df_comb['Fertility'] = df_comb.SexuponOutcome.str.split(expand=True)[0]\n",
    "\n",
    "age_dict = {'year':52,'years':52,'month':4.5,'months':4.5,'day':1/7,'days':1/7,'weeks':1,'week': 1}\n",
    "\n",
    "df_comb['Age_weeks'] =  df_comb.AgeuponOutcome.str.split().str[0].astype(float)*\\\n",
    "df_comb.AgeuponOutcome.str.split().str[1].map(age_dict)\n",
    "\n",
    "df_comb['Age_groups'],y = pd.qcut(df_comb['Age_weeks'],10,duplicates='drop',labels =False,retbins = True)   \n",
    "\n",
    "df_comb['Age_norm'] = (df_comb.Age_weeks - df_comb.Age_weeks.min())/(df_comb.Age_weeks.max() - df_comb.Age_weeks.min())\n",
    "\n",
    "df_comb['Age_std']  = (df_comb.Age_weeks - df_comb.Age_weeks.mean())/df_comb.Age_weeks.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns with the strings are mapped into integers using map function on respective columns. Also using datetime column several attributes like weekday , year, month have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb['Animal_map'] = df_comb['AnimalType'].map({'Dog':1,'Cat':0})\n",
    "\n",
    "df_comb['Sex_map'] = df_comb['Sex'].map({'Male':1,'Female':0})\n",
    "\n",
    "df_comb['Fertility_map'] = df_comb['Fertility'].map({'Neutered':0,'Spayed':0,'Intact':1})\n",
    "\n",
    "df_comb['Weekday'] = df_comb.DateTime.dt.dayofweek\n",
    "\n",
    "df_comb['Month'] = df_comb.DateTime.dt.month\n",
    "\n",
    "df_comb['Year'] = df_comb.DateTime.dt.year\n",
    "\n",
    "df_comb['Day_of_Year'] = df_comb.DateTime.dt.dayofyear\n",
    "\n",
    "df_comb['Month_end'] = df_comb.DateTime.dt.is_month_end*1\n",
    "\n",
    "df_comb['Month_start'] = df_comb.DateTime.dt.is_month_start*1\n",
    "\n",
    "df_comb['Quarter_start'] = df_comb.DateTime.dt.is_quarter_start*1\n",
    "\n",
    "df_comb['Quarter_end'] = df_comb.DateTime.dt.is_quarter_end*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For color and breed columns, the '/' is replaced with space and then each string is split into separate words using split function which gives output as a list of words. Now Output of all the strings is added to create a superset of all words contained in each column. Using the counter function on the list , we have extracted the top 20 frequently appearing words.\n",
    ".We have created the new column for each breed and color in these top 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('White', 11730),\n",
       " ('Black', 7439),\n",
       " ('Brown', 5654),\n",
       " ('Tan', 4370),\n",
       " ('Brindle', 1487),\n",
       " ('Red', 1396),\n",
       " ('Tricolor', 1279),\n",
       " ('Blue', 1216),\n",
       " ('Chocolate', 687),\n",
       " ('Sable', 459),\n",
       " ('Merle', 457),\n",
       " ('Buff', 431),\n",
       " ('Gray', 419),\n",
       " ('Yellow', 414),\n",
       " ('Cream', 397),\n",
       " ('Fawn', 294),\n",
       " ('Tick', 156),\n",
       " ('Silver', 113),\n",
       " ('Gold', 113),\n",
       " ('Apricot', 45)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dog = df_comb[df_comb.AnimalType=='Dog']\n",
    "print(df_dog.shape[0])\n",
    "Counter(df_dog.Color.str.replace('/',' ').str.split().sum()).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Tabby', 7404),\n",
       " ('White', 5632),\n",
       " ('Black', 4128),\n",
       " ('Brown', 3895),\n",
       " ('Blue', 2175),\n",
       " ('Orange', 1986),\n",
       " ('Tortie', 878),\n",
       " ('Point', 852),\n",
       " ('Calico', 802),\n",
       " ('Torbie', 567),\n",
       " ('Cream', 520),\n",
       " ('Lynx', 284),\n",
       " ('Seal', 236),\n",
       " ('Gray', 135),\n",
       " ('Flame', 122),\n",
       " ('Smoke', 110),\n",
       " ('Silver', 63),\n",
       " ('Lilac', 56),\n",
       " ('Chocolate', 54),\n",
       " ('Buff', 9)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat = df_comb[df_comb.AnimalType=='Cat']\n",
    "print(df_cat.shape[0])\n",
    "Counter(df_cat.Color.str.replace('/',' ').str.split().sum()).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['White','Black','Brown','Tan','Brindle','Red','Tricolor','Blue','Tabby','Orange','Tortie','Point','Calico','Torbie',\\\n",
    "         'Cream','Lynx','Seal','Gray','Flame','Smoke','Chocolate','Sable','Merle','Buff','Yellow','Tick','Silver','Gold',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in colors:\n",
    "    df_comb[x] = df_comb.Color.str.contains(x)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mix', 16330),\n",
       " ('Chihuahua', 3690),\n",
       " ('Retriever', 3539),\n",
       " ('Bull', 3529),\n",
       " ('Shorthair', 3460),\n",
       " ('Pit', 3458),\n",
       " ('Labrador', 3280),\n",
       " ('Terrier', 2435),\n",
       " ('Shepherd', 2001),\n",
       " ('Australian', 1516),\n",
       " ('German', 1427),\n",
       " ('Miniature', 1126),\n",
       " ('Dachshund', 1115),\n",
       " ('Dog', 996),\n",
       " ('Cattle', 905),\n",
       " ('Poodle', 716),\n",
       " ('Border', 713),\n",
       " ('Collie', 708),\n",
       " ('Boxer', 605),\n",
       " ('American', 541),\n",
       " ('Hound', 462),\n",
       " ('Beagle', 434),\n",
       " ('Russell', 430),\n",
       " ('Schnauzer', 404),\n",
       " ('Chow', 398),\n",
       " ('Jack', 389),\n",
       " ('Yorkshire', 383),\n",
       " ('Rat', 380),\n",
       " ('Catahoula', 371),\n",
       " ('Great', 352)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_dog.shape[0])\n",
    "Counter(df_dog.Breed.str.replace('/',' ').str.split().sum()).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_breed =['Mix','Chihuahua','Terrier','Retriever','Bull','Shepherd','Australian','German','Miniature',\\\n",
    "            'Cattle','Poodle','Border','Collie','Boxer','American','Hound','Beagle','Russell','Schnauzer','Chow','Jack',\\\n",
    "            'Yorkshire','Rat','Catahoula','Great','Labrador','Dachshund',]\n",
    "cat_breed = ['Domestic','Shorthair','Medium','Longhair','Siamese','Snowshoe']\n",
    "breeds = dog_breed + cat_breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in breeds:\n",
    "    df_comb[x] = df_comb.Breed.str.contains(x)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>AnimalID</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>ID</th>\n",
       "      <th>OutcomeSubtype</th>\n",
       "      <th>OutcomeType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>...</th>\n",
       "      <th>Catahoula</th>\n",
       "      <th>Great</th>\n",
       "      <th>Labrador</th>\n",
       "      <th>Dachshund</th>\n",
       "      <th>Domestic</th>\n",
       "      <th>Shorthair</th>\n",
       "      <th>Medium</th>\n",
       "      <th>Longhair</th>\n",
       "      <th>Siamese</th>\n",
       "      <th>Snowshoe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 year</td>\n",
       "      <td>A671945</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "      <td>Brown/White</td>\n",
       "      <td>2014-02-12 18:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Return_to_owner</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 year</td>\n",
       "      <td>A656520</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Cream Tabby</td>\n",
       "      <td>2013-10-13 12:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suffering</td>\n",
       "      <td>Euthanasia</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 years</td>\n",
       "      <td>A686464</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "      <td>Blue/White</td>\n",
       "      <td>2015-01-31 12:28:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Foster</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 weeks</td>\n",
       "      <td>A683430</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Blue Cream</td>\n",
       "      <td>2014-07-11 19:09:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 years</td>\n",
       "      <td>A667013</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "      <td>Tan</td>\n",
       "      <td>2013-11-15 12:52:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  AgeuponOutcome AnimalID AnimalType                        Breed  \\\n",
       "0         1 year  A671945        Dog        Shetland Sheepdog Mix   \n",
       "1         1 year  A656520        Cat       Domestic Shorthair Mix   \n",
       "2        2 years  A686464        Dog                 Pit Bull Mix   \n",
       "3        3 weeks  A683430        Cat       Domestic Shorthair Mix   \n",
       "4        2 years  A667013        Dog  Lhasa Apso/Miniature Poodle   \n",
       "\n",
       "         Color            DateTime  ID OutcomeSubtype      OutcomeType  \\\n",
       "0  Brown/White 2014-02-12 18:22:00 NaN            NaN  Return_to_owner   \n",
       "1  Cream Tabby 2013-10-13 12:44:00 NaN      Suffering       Euthanasia   \n",
       "2   Blue/White 2015-01-31 12:28:00 NaN         Foster         Adoption   \n",
       "3   Blue Cream 2014-07-11 19:09:00 NaN        Partner         Transfer   \n",
       "4          Tan 2013-11-15 12:52:00 NaN        Partner         Transfer   \n",
       "\n",
       "  SexuponOutcome    ...    Catahoula Great  Labrador  Dachshund  Domestic  \\\n",
       "0  Neutered Male    ...            0     0         0          0         0   \n",
       "1  Spayed Female    ...            0     0         0          0         1   \n",
       "2  Neutered Male    ...            0     0         0          0         0   \n",
       "3    Intact Male    ...            0     0         0          0         1   \n",
       "4  Neutered Male    ...            0     0         0          0         0   \n",
       "\n",
       "   Shorthair  Medium  Longhair  Siamese  Snowshoe  \n",
       "0          0       0         0        0         0  \n",
       "1          1       0         0        0         0  \n",
       "2          0       0         0        0         0  \n",
       "3          1       0         0        0         0  \n",
       "4          0       0         0        0         0  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnimalID          11456\n",
       "ID                26728\n",
       "OutcomeSubtype    25067\n",
       "OutcomeType       11456\n",
       "dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comb.isnull().sum()[df_comb.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the value of Outcome Type is known for values in training set, we have separated the combined dataset into training and test sets. Outcome type in training set is converted into integers using the map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_comb[df_comb['OutcomeType'].isnull()]\n",
    "df_raw = df_comb[~(df_comb['OutcomeType'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chitr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_raw['out_map'] = df_raw.OutcomeType.map({'Adoption':1,'Died':2,'Euthanasia':3, 'Transfer':4,'Return_to_owner':5 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ftr = ['Weekday','Month','Year','Day_of_Year','Month_end','Month_start','Quarter_start','Quarter_end',]\n",
    "features = ['Animal_map','Sex_map','Fertility_map','Age_weeks']+breeds+colors+date_ftr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a features_all list has been created which consists the names of all the features created. Using a subset of these features a logistic Regression Classifier is fitted and quality of fit is measured using log_loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_score = 1.18059     Validation_score = 1.17359\n",
      "Training_score = 1.17809     Validation_score = 1.17232\n",
      "Training_score = 1.03907     Validation_score = 1.03805\n",
      "Training_score = 0.97087     Validation_score = 0.97593\n",
      "Training_score = 0.97023     Validation_score = 0.97553\n",
      "Training_score = 0.96819     Validation_score = 0.9741\n",
      "Training_score = 0.96805     Validation_score = 0.97366\n",
      "Training_score = 0.96794     Validation_score = 0.9741\n",
      "Training_score = 0.96402     Validation_score = 0.96946\n",
      "Training_score = 0.96373     Validation_score = 0.96907\n",
      "Training_score = 0.96367     Validation_score = 0.96906\n",
      "Training_score = 0.9636     Validation_score = 0.96903\n",
      "Training_score = 0.96313     Validation_score = 0.9691\n",
      "Training_score = 0.96311     Validation_score = 0.96913\n",
      "Training_score = 0.96311     Validation_score = 0.96929\n",
      "Training_score = 0.9631     Validation_score = 0.96924\n",
      "Training_score = 0.96306     Validation_score = 0.96926\n",
      "Training_score = 0.96295     Validation_score = 0.96948\n",
      "Training_score = 0.96286     Validation_score = 0.96969\n",
      "Training_score = 0.96281     Validation_score = 0.96975\n",
      "Training_score = 0.96275     Validation_score = 0.96964\n",
      "Training_score = 0.96269     Validation_score = 0.96988\n",
      "Training_score = 0.96266     Validation_score = 0.97001\n",
      "Training_score = 0.96231     Validation_score = 0.96992\n",
      "Training_score = 0.96229     Validation_score = 0.96996\n",
      "Training_score = 0.96204     Validation_score = 0.97029\n",
      "Training_score = 0.96195     Validation_score = 0.97045\n",
      "Training_score = 0.96187     Validation_score = 0.97028\n",
      "Training_score = 0.9618     Validation_score = 0.97039\n",
      "Training_score = 0.96169     Validation_score = 0.97037\n",
      "Training_score = 0.9612     Validation_score = 0.96978\n",
      "Training_score = 0.96023     Validation_score = 0.96925\n",
      "Training_score = 0.95982     Validation_score = 0.96928\n",
      "Training_score = 0.95967     Validation_score = 0.96893\n",
      "Training_score = 0.95954     Validation_score = 0.96924\n",
      "Training_score = 0.95938     Validation_score = 0.96941\n",
      "Training_score = 0.95926     Validation_score = 0.96954\n",
      "Training_score = 0.95906     Validation_score = 0.96956\n",
      "Training_score = 0.95893     Validation_score = 0.96946\n",
      "Training_score = 0.95872     Validation_score = 0.96958\n",
      "Training_score = 0.95848     Validation_score = 0.96981\n",
      "Training_score = 0.95821     Validation_score = 0.97019\n",
      "Training_score = 0.95815     Validation_score = 0.97028\n",
      "Training_score = 0.95807     Validation_score = 0.97047\n",
      "Training_score = 0.9578     Validation_score = 0.97034\n",
      "Training_score = 0.95761     Validation_score = 0.97035\n",
      "Training_score = 0.9575     Validation_score = 0.97019\n",
      "Training_score = 0.95738     Validation_score = 0.97021\n",
      "Training_score = 0.95724     Validation_score = 0.97023\n",
      "Training_score = 0.95699     Validation_score = 0.97038\n",
      "Training_score = 0.95679     Validation_score = 0.97019\n",
      "Training_score = 0.95654     Validation_score = 0.97055\n",
      "Training_score = 0.95647     Validation_score = 0.9705\n",
      "Training_score = 0.95646     Validation_score = 0.97054\n",
      "Training_score = 0.95602     Validation_score = 0.97044\n",
      "Training_score = 0.95589     Validation_score = 0.97028\n",
      "Training_score = 0.95564     Validation_score = 0.96973\n",
      "Training_score = 0.95546     Validation_score = 0.9701\n",
      "Training_score = 0.95517     Validation_score = 0.96983\n",
      "Training_score = 0.95515     Validation_score = 0.96981\n",
      "Training_score = 0.95507     Validation_score = 0.96957\n",
      "Training_score = 0.95481     Validation_score = 0.96985\n",
      "Training_score = 0.95465     Validation_score = 0.96967\n",
      "Training_score = 0.95441     Validation_score = 0.96978\n",
      "Training_score = 0.95425     Validation_score = 0.9697\n",
      "Training_score = 0.9478     Validation_score = 0.96239\n",
      "Training_score = 0.94777     Validation_score = 0.96231\n",
      "Training_score = 0.95218     Validation_score = 0.96097\n",
      "Training_score = 0.95073     Validation_score = 0.96133\n",
      "Training_score = 0.94977     Validation_score = 0.9611\n",
      "Training_score = 0.94996     Validation_score = 0.96145\n",
      "Training_score = 0.95033     Validation_score = 0.96129\n",
      "Training_score = 0.95007     Validation_score = 0.96106\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features_all = (['Animal_map','Sex_map','Fertility_map','Age_groups']+breeds+colors+date_ftr)\n",
    "for i in range(len(features_all)):\n",
    "    features = features_all[:i+1]\n",
    "    df_trn,df_val,label_trn,label_val = train_test_split(df_raw[features],df_raw[['AnimalID','out_map']],\n",
    "                                                stratify = df_raw['out_map'].values,test_size = 0.3,random_state = 123)\n",
    "\n",
    "    #clf = RandomForestClassifier(n_estimators=100,max_features=0.5)\n",
    "    clf = LogisticRegression(penalty = 'l2',C=1)\n",
    "    clf.fit(df_trn.values, label_trn['out_map'].values)\n",
    "    \n",
    "    log_loss_prob_trn = log_loss(label_trn['out_map'].values,clf.predict_proba(df_trn.values),labels = [1,2,3,4,5])\n",
    "    log_loss_prob_val = log_loss(label_val['out_map'].values,clf.predict_proba(df_val.values),labels = [1,2,3,4,5])\n",
    "    \n",
    " \n",
    "    print('Training_score = '+ str(round(log_loss_prob_trn,5)) + '     Validation_score = ' + str(round(log_loss_prob_val,5)))\n",
    "    #print('Validation_score = ' + str(log_loss_prob_val))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the best features and tuning for penalty and C values, the best possible logistic classifier has been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_score = 0.95042     Validation_score = 0.96092\n"
     ]
    }
   ],
   "source": [
    "features = ['Animal_map', 'Sex_map', 'Fertility_map', 'Age_groups', 'Mix', 'Chihuahua', 'Labrador', 'Retriever', 'Bull',\\\n",
    "            'Shepherd', 'Australian', 'German', 'Miniature', 'Dachshund', 'Cattle', 'Poodle', 'Border', 'Collie', 'Boxer',\\\n",
    "            'American', 'Hound', 'Beagle', 'Russell', 'Schnauzer', 'Chow', 'Jack', 'Yorkshire', 'Rat', 'Catahoula', 'Great',\\\n",
    "            'Domestic', 'Shorthair', 'Medium', 'Longhair', 'Siamese', 'Snowshoe', 'White', 'Black', 'Brown', 'Tan', 'Brindle',\\\n",
    "            'Red', 'Tricolor', 'Blue', 'Tabby', 'Orange', 'Tortie', 'Point', 'Calico', 'Torbie', 'Cream', 'Lynx', 'Seal',\\\n",
    "            'Gray', 'Flame', 'Smoke', 'Chocolate', 'Sable', 'Merle', 'Buff', 'Yellow', 'Tick', 'Silver', 'Gold', 'Weekday',\\\n",
    "            'Month', 'Year', 'Day_of_Year', 'Month_end']\n",
    "df_trn,df_val,label_trn,label_val = train_test_split(df_raw[features],df_raw[['AnimalID','out_map']],\n",
    "                                            stratify = df_raw['out_map'].values,test_size = 0.3,random_state = 123)\n",
    "\n",
    "#clf = RandomForestClassifier(n_estimators=100,max_features=0.5)\n",
    "clf = LogisticRegression(penalty = 'l2',C=1)\n",
    "clf.fit(df_trn.values, label_trn['out_map'].values)\n",
    "\n",
    "log_loss_prob_trn = log_loss(label_trn['out_map'].values,clf.predict_proba(df_trn.values),labels = [1,2,3,4,5])\n",
    "log_loss_prob_val = log_loss(label_val['out_map'].values,clf.predict_proba(df_val.values),labels = [1,2,3,4,5])\n",
    "\n",
    "print('Training_score = '+ str(round(log_loss_prob_trn,5)) + '     Validation_score = ' + str(round(log_loss_prob_val,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the predict_proba method the probabilities for a feature to belong to different classes have been extracted and a dataframe has been created using those values. \n",
    "Finally using to_csv method on the dataframe, a csv value with the dataframe values has been created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cols=['Adoption','Died','Euthanasia', 'Transfer','Return_to_owner' ]\n",
    "\n",
    "#df_output = pd.get_dummies(clf.predict(df_tst.values))\n",
    "df_output = pd.DataFrame(clf.predict_proba(df_test[features]),columns=out_cols)\n",
    "df_output.columns = out_cols\n",
    "df_output['ID'] = (df_output.index+1)\n",
    "\n",
    "df_output.head()\n",
    "\n",
    "df_output.to_csv('logistic_best.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
